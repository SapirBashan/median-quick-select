{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBnegD1RfRunbCHPfqEW2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SapirBashan/median-quick-select/blob/main/simulator_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 2 of the simulator - text for test\n",
        "- channel nymber -> what channel\n",
        "- block size -> size of the block that needs to be copied\n",
        "- channel offset -> where in the channel the data needs to be copied to\n",
        "- data offset -> where in the data do i copy from\n",
        "\n",
        "'block = data[data + data_offset, data + data_offset + block_size]'\n",
        "\n",
        "'data_output[channel_number + block_offset,\n",
        "            channel_number + block_offset + block_size] = block'[link text](https://)\n"
      ],
      "metadata": {
        "id": "twTnIkjFHOYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Header:\n",
        "    def __init__(self, channel_number, block_size, channel_offset, data_offset, padding_1=0, padding_2=0):\n",
        "        self.padding_1 = padding_1\n",
        "        self.padding_2 = padding_2\n",
        "        self.channel_number = channel_number\n",
        "        self.block_size = block_size\n",
        "        self.channel_offset = channel_offset\n",
        "        self.data_offset = data_offset\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Header {self.channel_number}: padding_1={self.padding_1}, padding_2={self.padding_2}, \" \\\n",
        "               f\"channel_number={self.channel_number}, block_size={self.block_size}, \" \\\n",
        "               f\"channel_offset={self.channel_offset}, data_offset={self.data_offset}\"\n",
        "\n",
        "def create_test_data():\n",
        "    channels = 4\n",
        "    rows, cols = 5, 5\n",
        "    channel_size = rows * cols\n",
        "    data_cube = np.zeros((channels, rows, cols), dtype=np.uint32)\n",
        "\n",
        "    # Fill original data cube with unique values\n",
        "    for c in range(channels):\n",
        "        base = (c + 1) * 100\n",
        "        for r in range(rows):\n",
        "            for col in range(cols):\n",
        "                data_cube[c, r, col] = base + (r + 1) * 10 + (col + 1)\n",
        "\n",
        "    # Simulated shuffle plan: split every channel into pieces\n",
        "    #(x,y,z)\n",
        "    # x = channel number\n",
        "    # y = channel offset\n",
        "    # z = block size\n",
        "\n",
        "    def generate_random_shuffle_plan(data_cube):\n",
        "        channels, rows, cols = data_cube.shape\n",
        "        shuffle_plan = []\n",
        "        for channel in range(channels):\n",
        "            flat = data_cube[channel].flatten()\n",
        "            total = len(flat)\n",
        "            offset = 0\n",
        "            while offset < total:\n",
        "                remaining = total - offset\n",
        "                if remaining <= 3:\n",
        "                    block_size = remaining  # take all remaining elements\n",
        "                else:\n",
        "                    block_size = random.randint(3, min(10, remaining))\n",
        "                shuffle_plan.append((channel, offset, block_size))\n",
        "                offset += block_size\n",
        "        random.shuffle(shuffle_plan)  # Shuffle all chunks\n",
        "        return shuffle_plan\n",
        "\n",
        "\n",
        "    shuffle_plan = generate_random_shuffle_plan(data_cube)\n",
        "\n",
        "    shuffled_data = []\n",
        "    headers = []\n",
        "    data_offset = 0\n",
        "\n",
        "    for (channel_number, channel_offset, block_size) in shuffle_plan:\n",
        "        channel_flat = data_cube[channel_number].flatten()\n",
        "        block_data = channel_flat[channel_offset: channel_offset + block_size]\n",
        "        shuffled_data.extend(block_data)\n",
        "\n",
        "        header = Header(\n",
        "            channel_number=channel_number,\n",
        "            block_size=block_size,\n",
        "            channel_offset=channel_offset,\n",
        "            data_offset=data_offset\n",
        "        )\n",
        "        headers.append(header)\n",
        "        data_offset += block_size\n",
        "\n",
        "    shuffled_data = np.array(shuffled_data, dtype=np.uint32)\n",
        "\n",
        "    # Assemble: initialize flat arrays per channel\n",
        "    assembled = np.zeros_like(data_cube)\n",
        "    assembled_flat = {ch: assembled[ch].flatten() for ch in range(channels)}\n",
        "\n",
        "    for header in headers:\n",
        "        block = shuffled_data[header.data_offset: header.data_offset + header.block_size]\n",
        "        dst = assembled_flat[header.channel_number]\n",
        "        dst[header.channel_offset: header.channel_offset + header.block_size] = block\n",
        "\n",
        "    # Reshape\n",
        "    for ch in range(channels):\n",
        "        assembled[ch] = assembled_flat[ch].reshape(rows, cols)\n",
        "\n",
        "    return data_cube, shuffled_data, headers, assembled\n",
        "\n",
        "def write_test_files():\n",
        "    original, shuffled_data, headers, assembled = create_test_data()\n",
        "\n",
        "    # 1. Write original\n",
        "    with open('original_data_cube.txt', 'w') as f:\n",
        "        for c in range(original.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(original.shape[1]):\n",
        "                row = ' '.join(f\"{original[c, r, col]:4d}\" for col in range(original.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # 2. Write shuffled\n",
        "    with open('shuffled_data.txt', 'w') as f:\n",
        "        f.write(\"Shuffled Data Stream:\\n\")\n",
        "        for i in range(0, len(shuffled_data), 5):\n",
        "            chunk = shuffled_data[i:i+5]\n",
        "            values = ' '.join(f\"{v:4d}\" for v in chunk)\n",
        "            f.write(f\"  {values}\\n\")\n",
        "\n",
        "    # 3. Write headers sorted by channel\n",
        "    with open('headers.txt', 'w') as f:\n",
        "        #sorted_headers = sorted(headers, key=lambda h: (h.channel_number, h.channel_offset))\n",
        "        for h in headers:\n",
        "            f.write(str(h) + '\\n')\n",
        "\n",
        "    # 4. Write assembled\n",
        "    with open('assembled_data_cube.txt', 'w') as f:\n",
        "        for c in range(assembled.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(assembled.shape[1]):\n",
        "                row = ' '.join(f\"{assembled[c, r, col]:4d}\" for col in range(assembled.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Check\n",
        "    print(\"âœ… Files created.\")\n",
        "    if np.array_equal(original, assembled):\n",
        "        print(\"âœ… Reconstruction MATCHES original!\")\n",
        "    else:\n",
        "        print(\"âŒ Reconstruction failed â€” mismatch detected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    write_test_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7veDLAAZHSnf",
        "outputId": "1f952c6d-8980-4b16-9786-e6f8a8777704"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Files created.\n",
            "âœ… Reconstruction MATCHES original!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Changes:\n",
        "Max block size is now 64KB (16,384 uint32 values).\n",
        "\n",
        "Binary file writing/reading added.\n",
        "\n",
        "Cube size is 96 channels Ã— 1024 Ã— 1024.\n",
        "\n",
        "Still performs reconstruction check.\n",
        "\n"
      ],
      "metadata": {
        "id": "o86qYsP8nSEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import struct\n",
        "\n",
        "MAX_BLOCK_SIZE = 64 * 1024  # 64 KB\n",
        "MAX_UINT32_VALUES_PER_BLOCK = MAX_BLOCK_SIZE // 4  # 16384\n",
        "\n",
        "class Header:\n",
        "    def __init__(self, channel_number, block_size, channel_offset, data_offset, padding_1=0, padding_2=0):\n",
        "        self.padding_1 = padding_1\n",
        "        self.padding_2 = padding_2\n",
        "        self.channel_number = channel_number\n",
        "        self.block_size = block_size\n",
        "        self.channel_offset = channel_offset\n",
        "        self.data_offset = data_offset\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Header {self.channel_number}: padding_1={self.padding_1}, padding_2={self.padding_2}, \" \\\n",
        "               f\"channel_number={self.channel_number}, block_size={self.block_size}, \" \\\n",
        "               f\"channel_offset={self.channel_offset}, data_offset={self.data_offset}\"\n",
        "\n",
        "def create_large_test_data():\n",
        "    channels, rows, cols = 96, 1024, 1024\n",
        "    data_cube = np.zeros((channels, rows, cols), dtype=np.uint32)\n",
        "\n",
        "    print(\"ðŸ”¢ Filling data cube...\")\n",
        "    for c in range(channels):\n",
        "        base = (c + 1) * 1_000_000\n",
        "        data_cube[c] = base + np.arange(rows * cols, dtype=np.uint32).reshape(rows, cols)\n",
        "\n",
        "    print(\"ðŸ”€ Generating shuffle plan...\")\n",
        "\n",
        "    def generate_random_shuffle_plan(data_cube):\n",
        "        channels, rows, cols = data_cube.shape\n",
        "        shuffle_plan = []\n",
        "        for channel in range(channels):\n",
        "            flat = data_cube[channel].flatten()\n",
        "            total = len(flat)\n",
        "            offset = 0\n",
        "            while offset < total:\n",
        "                remaining = total - offset\n",
        "                if remaining <= 3:\n",
        "                    block_size = remaining\n",
        "                else:\n",
        "                    block_size = random.randint(3, min(MAX_UINT32_VALUES_PER_BLOCK, remaining))\n",
        "                shuffle_plan.append((channel, offset, block_size))\n",
        "                offset += block_size\n",
        "        random.shuffle(shuffle_plan)\n",
        "        return shuffle_plan\n",
        "\n",
        "    shuffle_plan = generate_random_shuffle_plan(data_cube)\n",
        "\n",
        "    shuffled_data = []\n",
        "    headers = []\n",
        "    data_offset = 0\n",
        "\n",
        "    for (channel_number, channel_offset, block_size) in shuffle_plan:\n",
        "        channel_flat = data_cube[channel_number].flatten()\n",
        "        block_data = channel_flat[channel_offset: channel_offset + block_size]\n",
        "        shuffled_data.extend(block_data)\n",
        "\n",
        "        header = Header(\n",
        "            channel_number=channel_number,\n",
        "            block_size=block_size,\n",
        "            channel_offset=channel_offset,\n",
        "            data_offset=data_offset\n",
        "        )\n",
        "        headers.append(header)\n",
        "        data_offset += block_size\n",
        "\n",
        "    return data_cube, np.array(shuffled_data, dtype=np.uint32), headers\n",
        "\n",
        "def write_binary_and_test():\n",
        "    original, shuffled_data, headers = create_large_test_data()\n",
        "\n",
        "    print(\"ðŸ’¾ Writing shuffled binary data...\")\n",
        "    with open('shuffled_data.bin', 'wb') as f:\n",
        "        shuffled_data.tofile(f)\n",
        "\n",
        "\n",
        "    print(\"ðŸ’¾ Writing headers to binary file...\")\n",
        "    with open('headers.bin', 'wb') as f:\n",
        "        for h in headers:\n",
        "            packed = struct.pack(\n",
        "                '6I',  # 6 unsigned 32-bit integers\n",
        "                h.padding_1,\n",
        "                h.padding_2,\n",
        "                h.channel_number,\n",
        "                h.block_size,\n",
        "                h.channel_offset,\n",
        "                h.data_offset\n",
        "            )\n",
        "            f.write(packed)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"ðŸ” Reconstructing...\")\n",
        "    assembled = np.zeros_like(original)\n",
        "    assembled_flat = {ch: assembled[ch].flatten() for ch in range(original.shape[0])}\n",
        "\n",
        "    for header in headers:\n",
        "        block = shuffled_data[header.data_offset: header.data_offset + header.block_size]\n",
        "        dst = assembled_flat[header.channel_number]\n",
        "        dst[header.channel_offset: header.channel_offset + header.block_size] = block\n",
        "\n",
        "    for ch in range(original.shape[0]):\n",
        "        assembled[ch] = assembled_flat[ch].reshape(original.shape[1], original.shape[2])\n",
        "\n",
        "    print(\"âœ… Files created.\")\n",
        "    if np.array_equal(original, assembled):\n",
        "        print(\"âœ… Reconstruction MATCHES original!\")\n",
        "    else:\n",
        "        print(\"âŒ Reconstruction failed â€” mismatch detected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    write_binary_and_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEv3hsrPkWRr",
        "outputId": "50f2f1cd-187e-4f99-9ca9-5356b4b52d96"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¢ Filling data cube...\n",
            "ðŸ”€ Generating shuffle plan...\n",
            "ðŸ’¾ Writing shuffled binary data...\n",
            "ðŸ’¾ Writing headers to binary file...\n",
            "ðŸ” Reconstructing...\n",
            "âœ… Files created.\n",
            "âœ… Reconstruction MATCHES original!\n"
          ]
        }
      ]
    }
  ]
}