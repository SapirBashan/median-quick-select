{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTJ4w+KCFbqj9vSvfEbeki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SapirBashan/median-quick-select/blob/main/simulator_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle channel implementaion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8JJn-0x0WbOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "import os\n",
        "\n",
        "# Define header structure\n",
        "class Header:\n",
        "    def __init__(self, channel_number, block_size=16384, padding_1=0, padding_2=0):\n",
        "        self.padding_1 = padding_1\n",
        "        self.padding_2 = padding_2\n",
        "        self.channel_number = channel_number\n",
        "        self.block_size = block_size  # 64 kB = 16k values of uint32\n",
        "        self.channel_offset = channel_number * block_size * 4  # 4 bytes per uint32\n",
        "        self.data_offset = channel_number * block_size\n",
        "\n",
        "    def pack(self):\n",
        "        return struct.pack('<HHHIII',\n",
        "                          self.padding_1,\n",
        "                          self.padding_2,\n",
        "                          self.channel_number,\n",
        "                          self.block_size,\n",
        "                          self.channel_offset,\n",
        "                          self.data_offset)\n",
        "\n",
        "    @staticmethod\n",
        "    def size():\n",
        "        return struct.calcsize('<HHHIII')\n",
        "\n",
        "    @staticmethod\n",
        "    def unpack(data):\n",
        "        unpacked = struct.unpack('<HHHIII', data)\n",
        "        header = Header(channel_number=unpacked[2])\n",
        "        header.padding_1 = unpacked[0]\n",
        "        header.padding_2 = unpacked[1]\n",
        "        header.block_size = unpacked[3]\n",
        "        header.channel_offset = unpacked[4]\n",
        "        header.data_offset = unpacked[5]\n",
        "        return header\n",
        "\n",
        "def generate_data_cube(num_channels=96, vector_count=1024, vector_length=1024):\n",
        "    \"\"\"Generate a data cube with sequential values for testing\"\"\"\n",
        "    data_cube = np.zeros((num_channels, vector_count, vector_length), dtype=np.uint32)\n",
        "\n",
        "    # Fill with sequential values for easier validation\n",
        "    value = 1\n",
        "    for c in range(num_channels):\n",
        "        for i in range(vector_count):\n",
        "            for j in range(vector_length):\n",
        "                data_cube[c, i, j] = value\n",
        "                value += 1\n",
        "\n",
        "    return data_cube\n",
        "\n",
        "def data_cube_to_shuffled_data(data_cube):\n",
        "    \"\"\"Convert data cube to shuffled data and headers\"\"\"\n",
        "    num_channels, vector_count, vector_length = data_cube.shape\n",
        "\n",
        "    # Create headers\n",
        "    headers = []\n",
        "    for channel in range(num_channels):\n",
        "        headers.append(Header(channel_number=channel))\n",
        "\n",
        "    # Create shuffled data array\n",
        "    total_elements = num_channels * vector_count * vector_length\n",
        "    shuffled_data = np.zeros(total_elements, dtype=np.uint32)\n",
        "\n",
        "    # Determine random permutation of blocks\n",
        "    block_indices = np.random.permutation(num_channels)\n",
        "\n",
        "    # Fill shuffled data array\n",
        "    pos = 0\n",
        "    for block_idx in block_indices:\n",
        "        block_data = data_cube[block_idx].flatten()\n",
        "        block_size = len(block_data)\n",
        "        shuffled_data[pos:pos+block_size] = block_data\n",
        "\n",
        "        # Update header with the position in the shuffled data\n",
        "        headers[block_idx].data_offset = pos\n",
        "        pos += block_size\n",
        "\n",
        "    return shuffled_data, headers\n",
        "\n",
        "def save_data_files(data_cube, shuffled_data, headers, dimensions=None):\n",
        "    \"\"\"Save data cube, shuffled data, and headers to binary files\"\"\"\n",
        "    if dimensions is None:\n",
        "        dimensions = data_cube.shape\n",
        "\n",
        "    # Save data cube\n",
        "    with open('data_cube.bin', 'wb') as f:\n",
        "        f.write(data_cube.tobytes())\n",
        "\n",
        "    # Save shuffled data\n",
        "    with open('data.bin', 'wb') as f:\n",
        "        f.write(shuffled_data.tobytes())\n",
        "\n",
        "    # Save headers\n",
        "    with open('headers.bin', 'wb') as f:\n",
        "        for header in headers:\n",
        "            f.write(header.pack())\n",
        "\n",
        "    # Save dimensions metadata for easier loading\n",
        "    with open('dimensions.txt', 'w') as f:\n",
        "        f.write(f\"{dimensions[0]} {dimensions[1]} {dimensions[2]}\")\n",
        "\n",
        "def load_data_files():\n",
        "    \"\"\"Load data from binary files\"\"\"\n",
        "    # Load dimensions\n",
        "    if os.path.exists('dimensions.txt'):\n",
        "        with open('dimensions.txt', 'r') as f:\n",
        "            dims = f.read().strip().split()\n",
        "            num_channels = int(dims[0])\n",
        "            vector_count = int(dims[1])\n",
        "            vector_length = int(dims[2])\n",
        "    else:\n",
        "        # Default dimensions\n",
        "        num_channels = 96\n",
        "        vector_count = 1024\n",
        "        vector_length = 1024\n",
        "\n",
        "    # Load data cube\n",
        "    data_cube_size = os.path.getsize('data_cube.bin')\n",
        "    expected_size = num_channels * vector_count * vector_length * 4  # 4 bytes per uint32\n",
        "\n",
        "    if data_cube_size != expected_size:\n",
        "        print(f\"Warning: data_cube.bin size {data_cube_size} does not match expected size {expected_size}\")\n",
        "        # Try to deduce dimensions\n",
        "        total_elements = data_cube_size // 4  # 4 bytes per uint32\n",
        "        if os.path.exists('dimensions.txt'):\n",
        "            print(f\"Using dimensions from dimensions.txt: {num_channels}×{vector_count}×{vector_length}\")\n",
        "        else:\n",
        "            print(\"Unable to determine correct dimensions, using default\")\n",
        "\n",
        "    with open('data_cube.bin', 'rb') as f:\n",
        "        data_cube_bytes = f.read(data_cube_size)\n",
        "        try:\n",
        "            data_cube = np.frombuffer(data_cube_bytes, dtype=np.uint32).reshape(num_channels, vector_count, vector_length)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error reshaping data cube: {e}\")\n",
        "            total_elements = len(np.frombuffer(data_cube_bytes, dtype=np.uint32))\n",
        "            print(f\"Total elements: {total_elements}\")\n",
        "            # Try to guess dimensions\n",
        "            if total_elements == 36:  # 4×3×3\n",
        "                num_channels, vector_count, vector_length = 4, 3, 3\n",
        "            elif total_elements == 27:  # 3×3×3\n",
        "                num_channels, vector_count, vector_length = 3, 3, 3\n",
        "            else:\n",
        "                raise ValueError(f\"Cannot determine dimensions for {total_elements} elements\")\n",
        "\n",
        "            data_cube = np.frombuffer(data_cube_bytes, dtype=np.uint32).reshape(num_channels, vector_count, vector_length)\n",
        "\n",
        "    # Load shuffled data\n",
        "    with open('data.bin', 'rb') as f:\n",
        "        data_bytes = f.read()\n",
        "        shuffled_data = np.frombuffer(data_bytes, dtype=np.uint32)\n",
        "\n",
        "    # Load headers\n",
        "    header_size = Header.size()\n",
        "    num_headers = os.path.getsize('headers.bin') // header_size\n",
        "    headers = []\n",
        "\n",
        "    with open('headers.bin', 'rb') as f:\n",
        "        for _ in range(num_headers):\n",
        "            header_bytes = f.read(header_size)\n",
        "            headers.append(Header.unpack(header_bytes))\n",
        "\n",
        "    return data_cube, shuffled_data, headers, (num_channels, vector_count, vector_length)\n",
        "\n",
        "def reconstruct_data_cube(shuffled_data, headers, vector_count=1024, vector_length=1024):\n",
        "    \"\"\"Reconstruct the data cube from shuffled data and headers\"\"\"\n",
        "    num_channels = len(headers)\n",
        "    reconstructed_cube = np.zeros((num_channels, vector_count, vector_length), dtype=np.uint32)\n",
        "\n",
        "    for header in headers:\n",
        "        channel = header.channel_number\n",
        "        offset = header.data_offset\n",
        "        block_size = vector_count * vector_length\n",
        "\n",
        "        # Extract data from shuffled array\n",
        "        channel_data = shuffled_data[offset:offset + block_size]\n",
        "        reconstructed_cube[channel] = channel_data.reshape(vector_count, vector_length)\n",
        "\n",
        "    return reconstructed_cube\n",
        "\n",
        "def verify_reconstruction(original_cube, reconstructed_cube):\n",
        "    \"\"\"Verify that the reconstructed cube matches the original\"\"\"\n",
        "    if not np.array_equal(original_cube, reconstructed_cube):\n",
        "        print(\"❌ Reconstruction FAILED\")\n",
        "        mismatches = np.where(original_cube != reconstructed_cube)\n",
        "        print(f\"Found {len(mismatches[0])} mismatches\")\n",
        "        for i in range(min(10, len(mismatches[0]))):\n",
        "            idx = (mismatches[0][i], mismatches[1][i], mismatches[2][i])\n",
        "            print(f\"Mismatch at {idx}: Original={original_cube[idx]}, Reconstructed={reconstructed_cube[idx]}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ Reconstruction successful - All values match!\")\n",
        "        return True\n",
        "\n",
        "def main():\n",
        "    # Use smaller dimensions for demonstration\n",
        "    test_channels = 4  # Smaller for demonstration, use 96 for actual\n",
        "    test_vector_count = 3  # Smaller for demonstration, use 1024 for actual\n",
        "    test_vector_length = 3  # Smaller for demonstration, use 1024 for actual\n",
        "\n",
        "    print(\"Generating data cube...\")\n",
        "    data_cube = generate_data_cube(test_channels, test_vector_count, test_vector_length)\n",
        "    print(f\"Data cube shape: {data_cube.shape}\")\n",
        "\n",
        "    # Show a sample of the data cube for visualization\n",
        "    print(\"\\nSample of data cube (first 2 channels):\")\n",
        "    for c in range(min(2, test_channels)):\n",
        "        print(f\"\\nChannel {c}:\")\n",
        "        print(data_cube[c])\n",
        "\n",
        "    print(\"\\nShuffling data and creating headers...\")\n",
        "    shuffled_data, headers = data_cube_to_shuffled_data(data_cube)\n",
        "\n",
        "    print(f\"Shuffled data length: {len(shuffled_data)}\")\n",
        "    print(f\"Number of headers: {len(headers)}\")\n",
        "\n",
        "    print(\"\\nSaving files...\")\n",
        "    save_data_files(data_cube, shuffled_data, headers, (test_channels, test_vector_count, test_vector_length))\n",
        "\n",
        "    print(\"\\nLoading files...\")\n",
        "    loaded_cube, loaded_data, loaded_headers, dimensions = load_data_files()\n",
        "\n",
        "    print(f\"Loaded data cube shape: {loaded_cube.shape}\")\n",
        "\n",
        "    print(\"\\nReconstructing data cube from loaded files...\")\n",
        "    reconstructed_cube = reconstruct_data_cube(loaded_data, loaded_headers, dimensions[1], dimensions[2])\n",
        "\n",
        "    print(\"\\nVerifying reconstruction...\")\n",
        "    success = verify_reconstruction(loaded_cube, reconstructed_cube)\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nDo you want to run the full-scale test (96×1024×1024)? (y/n)\")\n",
        "        response = input().strip().lower()\n",
        "        if response == 'y':\n",
        "            print(\"\\nFull-scale test\")\n",
        "            print(\"Generating full 96×1024×1024 data cube...\")\n",
        "            data_cube_full = generate_data_cube()\n",
        "            shuffled_data_full, headers_full = data_cube_to_shuffled_data(data_cube_full)\n",
        "            save_data_files(data_cube_full, shuffled_data_full, headers_full)\n",
        "            print(\"Full-scale files generated successfully.\")\n",
        "        else:\n",
        "            print(\"Skipping full-scale test.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL-hGdxx4VI8",
        "outputId": "f5857c44-b4ba-42bd-8a2d-767072b3140d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data cube...\n",
            "Data cube shape: (4, 3, 3)\n",
            "\n",
            "Sample of data cube (first 2 channels):\n",
            "\n",
            "Channel 0:\n",
            "[[1 2 3]\n",
            " [4 5 6]\n",
            " [7 8 9]]\n",
            "\n",
            "Channel 1:\n",
            "[[10 11 12]\n",
            " [13 14 15]\n",
            " [16 17 18]]\n",
            "\n",
            "Shuffling data and creating headers...\n",
            "Shuffled data length: 36\n",
            "Number of headers: 4\n",
            "\n",
            "Saving files...\n",
            "\n",
            "Loading files...\n",
            "Loaded data cube shape: (4, 3, 3)\n",
            "\n",
            "Reconstructing data cube from loaded files...\n",
            "\n",
            "Verifying reconstruction...\n",
            "✅ Reconstruction successful - All values match!\n",
            "\n",
            "Do you want to run the full-scale test (96×1024×1024)? (y/n)\n",
            "y\n",
            "\n",
            "Full-scale test\n",
            "Generating full 96×1024×1024 data cube...\n",
            "Full-scale files generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle channel implementaion test with stall text data"
      ],
      "metadata": {
        "id": "Kam9-ubRWitX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "import os\n",
        "\n",
        "# Define header structure as in the original code\n",
        "class Header:\n",
        "    def __init__(self, channel_number, block_size=16, padding_1=0, padding_2=0):\n",
        "        self.padding_1 = padding_1\n",
        "        self.padding_2 = padding_2\n",
        "        self.channel_number = channel_number\n",
        "        self.block_size = block_size\n",
        "        self.channel_offset = channel_number * block_size * 4  # 4 bytes per uint32\n",
        "        self.data_offset = channel_number * block_size\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Header {self.channel_number}: padding_1={self.padding_1}, padding_2={self.padding_2}, \" \\\n",
        "               f\"channel_number={self.channel_number}, block_size={self.block_size}, \" \\\n",
        "               f\"channel_offset={self.channel_offset}, data_offset={self.data_offset}\"\n",
        "\n",
        "# Create a small test data cube: 3 channels x 4 rows x 4 columns\n",
        "def create_test_data():\n",
        "    channels = 3\n",
        "    rows = 4\n",
        "    cols = 4\n",
        "\n",
        "    # Create data cube with easily identifiable values\n",
        "    data_cube = np.zeros((channels, rows, cols), dtype=np.uint32)\n",
        "\n",
        "    # Fill with values that clearly identify channel, row, column\n",
        "    for c in range(channels):\n",
        "        base = (c + 1) * 100  # Channel 0: 100s, Channel 1: 200s, Channel 2: 300s\n",
        "        for r in range(rows):\n",
        "            for col in range(cols):\n",
        "                # Value format: (channel+1)(row+1)(col+1)\n",
        "                data_cube[c, r, col] = base + (r + 1) * 10 + (col + 1)\n",
        "\n",
        "    # Create headers\n",
        "    headers = []\n",
        "    for c in range(channels):\n",
        "        headers.append(Header(channel_number=c, block_size=rows*cols))\n",
        "\n",
        "    # Create shuffled data in a specific order for testing\n",
        "    # We'll use a simple shuffling pattern: channel 2, then 0, then 1\n",
        "    shuffle_order = [2, 0, 1]\n",
        "    shuffled_data = np.zeros(channels * rows * cols, dtype=np.uint32)\n",
        "\n",
        "    pos = 0\n",
        "    for c in shuffle_order:\n",
        "        block_data = data_cube[c].flatten()\n",
        "        block_size = len(block_data)\n",
        "        shuffled_data[pos:pos+block_size] = block_data\n",
        "\n",
        "        # Update header with the position in the shuffled data\n",
        "        headers[c].data_offset = pos\n",
        "        pos += block_size\n",
        "\n",
        "    # Reconstruct data cube from shuffled data and headers\n",
        "    reconstructed = np.zeros_like(data_cube)\n",
        "    for header in headers:\n",
        "        channel = header.channel_number\n",
        "        offset = header.data_offset\n",
        "        block_size = rows * cols\n",
        "        channel_data = shuffled_data[offset:offset + block_size]\n",
        "        reconstructed[channel] = channel_data.reshape(rows, cols)\n",
        "\n",
        "    return data_cube, shuffled_data, headers, reconstructed\n",
        "\n",
        "def write_test_files():\n",
        "    data_cube, shuffled_data, headers, reconstructed = create_test_data()\n",
        "\n",
        "    # Write headers to text file\n",
        "    with open('headers.txt', 'w') as f:\n",
        "        for header in headers:\n",
        "            f.write(str(header) + '\\n')\n",
        "\n",
        "    # Write original data cube to text file\n",
        "    with open('original_data_cube.txt', 'w') as f:\n",
        "        for c in range(data_cube.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(data_cube.shape[1]):\n",
        "                row_values = ' '.join(f\"{data_cube[c, r, col]:4d}\" for col in range(data_cube.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row_values}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Write shuffled data to text file\n",
        "    with open('data.txt', 'w') as f:\n",
        "        f.write(\"Shuffled data array:\\n\")\n",
        "        for i in range(0, len(shuffled_data), data_cube.shape[2]):\n",
        "            chunk = shuffled_data[i:i+data_cube.shape[2]]\n",
        "            values = ' '.join(f\"{val:4d}\" for val in chunk)\n",
        "            f.write(f\"  {values}\\n\")\n",
        "\n",
        "    # Write reconstructed data cube to text file\n",
        "    with open('recon_data_cube.txt', 'w') as f:\n",
        "        for c in range(reconstructed.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(reconstructed.shape[1]):\n",
        "                row_values = ' '.join(f\"{reconstructed[c, r, col]:4d}\" for col in range(reconstructed.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row_values}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Print summary\n",
        "    print(\"Test files created:\")\n",
        "    print(\"1. headers.txt - Contains header information\")\n",
        "    print(\"2. original_data_cube.txt - Original 3×4×4 data cube\")\n",
        "    print(\"3. data.txt - Shuffled data array\")\n",
        "    print(\"4. recon_data_cube.txt - Reconstructed data cube from shuffled data\")\n",
        "\n",
        "    # check if the original data_cube is equel to the reconstroction\n",
        "    if np.array_equal(data_cube, reconstructed):\n",
        "        print(\"✅ Reconstruction successful - All values match!\")\n",
        "    else:\n",
        "        print(\"❌ Reconstruction FAILED\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    write_test_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ9FNeGl6TAJ",
        "outputId": "be4a2565-c997-4c26-81dd-bd5500d8c270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test files created:\n",
            "1. headers.txt - Contains header information\n",
            "2. original_data_cube.txt - Original 3×4×4 data cube\n",
            "3. data.txt - Shuffled data array\n",
            "4. recon_data_cube.txt - Reconstructed data cube from shuffled data\n",
            "✅ Reconstruction successful - All values match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# version 2 of the simulator - text for test\n",
        "- channel nymber -> what channel\n",
        "- block size -> size of the block that needs to be copied\n",
        "- channel offset -> where in the channel the data needs to be copied to\n",
        "- data offset -> where in the data do i copy from\n",
        "\n",
        "'block = data[data + data_offset, data + data_offset + block_size]'\n",
        "\n",
        "'data_output[channel_number + block_offset,\n",
        "            channel_number + block_offset + block_size] = block'[link text](https://)\n"
      ],
      "metadata": {
        "id": "twTnIkjFHOYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Header:\n",
        "    def __init__(self, channel_number, block_size, channel_offset, data_offset, padding_1=0, padding_2=0):\n",
        "        self.padding_1 = padding_1\n",
        "        self.padding_2 = padding_2\n",
        "        self.channel_number = channel_number\n",
        "        self.block_size = block_size\n",
        "        self.channel_offset = channel_offset\n",
        "        self.data_offset = data_offset\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Header {self.channel_number}: padding_1={self.padding_1}, padding_2={self.padding_2}, \" \\\n",
        "               f\"channel_number={self.channel_number}, block_size={self.block_size}, \" \\\n",
        "               f\"channel_offset={self.channel_offset}, data_offset={self.data_offset}\"\n",
        "\n",
        "def create_test_data():\n",
        "    channels = 4\n",
        "    rows, cols = 5, 5\n",
        "    channel_size = rows * cols\n",
        "    data_cube = np.zeros((channels, rows, cols), dtype=np.uint32)\n",
        "\n",
        "    # Fill original data cube with unique values\n",
        "    for c in range(channels):\n",
        "        base = (c + 1) * 100\n",
        "        for r in range(rows):\n",
        "            for col in range(cols):\n",
        "                data_cube[c, r, col] = base + (r + 1) * 10 + (col + 1)\n",
        "\n",
        "    # Simulated shuffle plan: split every channel into 2 pieces\n",
        "    shuffle_plan = [\n",
        "        (2, 0, 12),\n",
        "        (0, 0, 5),\n",
        "        (1, 0, 8),\n",
        "        (3, 0, 13),\n",
        "        (2, 12, 13),\n",
        "        (1, 8, 17),\n",
        "        (0, 5, 20),\n",
        "        (3, 13, 12),\n",
        "    ]\n",
        "\n",
        "    shuffled_data = []\n",
        "    headers = []\n",
        "    data_offset = 0\n",
        "\n",
        "    for (channel_number, channel_offset, block_size) in shuffle_plan:\n",
        "        channel_flat = data_cube[channel_number].flatten()\n",
        "        block_data = channel_flat[channel_offset: channel_offset + block_size]\n",
        "        shuffled_data.extend(block_data)\n",
        "\n",
        "        header = Header(\n",
        "            channel_number=channel_number,\n",
        "            block_size=block_size,\n",
        "            channel_offset=channel_offset,\n",
        "            data_offset=data_offset\n",
        "        )\n",
        "        headers.append(header)\n",
        "        data_offset += block_size\n",
        "\n",
        "    shuffled_data = np.array(shuffled_data, dtype=np.uint32)\n",
        "\n",
        "    # Assemble: initialize flat arrays per channel\n",
        "    assembled = np.zeros_like(data_cube)\n",
        "    assembled_flat = {ch: assembled[ch].flatten() for ch in range(channels)}\n",
        "\n",
        "    for header in headers:\n",
        "        block = shuffled_data[header.data_offset: header.data_offset + header.block_size]\n",
        "        dst = assembled_flat[header.channel_number]\n",
        "        dst[header.channel_offset: header.channel_offset + header.block_size] = block\n",
        "\n",
        "    # Reshape\n",
        "    for ch in range(channels):\n",
        "        assembled[ch] = assembled_flat[ch].reshape(rows, cols)\n",
        "\n",
        "    return data_cube, shuffled_data, headers, assembled\n",
        "\n",
        "def write_test_files():\n",
        "    original, shuffled_data, headers, assembled = create_test_data()\n",
        "\n",
        "    # 1. Write original\n",
        "    with open('original_data_cube.txt', 'w') as f:\n",
        "        for c in range(original.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(original.shape[1]):\n",
        "                row = ' '.join(f\"{original[c, r, col]:4d}\" for col in range(original.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # 2. Write shuffled\n",
        "    with open('shuffled_data.txt', 'w') as f:\n",
        "        f.write(\"Shuffled Data Stream:\\n\")\n",
        "        for i in range(0, len(shuffled_data), 5):\n",
        "            chunk = shuffled_data[i:i+5]\n",
        "            values = ' '.join(f\"{v:4d}\" for v in chunk)\n",
        "            f.write(f\"  {values}\\n\")\n",
        "\n",
        "    # 3. Write headers sorted by channel\n",
        "    with open('headers.txt', 'w') as f:\n",
        "        #sorted_headers = sorted(headers, key=lambda h: (h.channel_number, h.channel_offset))\n",
        "        for h in headers:\n",
        "            f.write(str(h) + '\\n')\n",
        "\n",
        "    # 4. Write assembled\n",
        "    with open('assembled_data_cube.txt', 'w') as f:\n",
        "        for c in range(assembled.shape[0]):\n",
        "            f.write(f\"Channel {c}:\\n\")\n",
        "            for r in range(assembled.shape[1]):\n",
        "                row = ' '.join(f\"{assembled[c, r, col]:4d}\" for col in range(assembled.shape[2]))\n",
        "                f.write(f\"  Row {r}: {row}\\n\")\n",
        "            f.write('\\n')\n",
        "\n",
        "    # Check\n",
        "    print(\"✅ Files created.\")\n",
        "    if np.array_equal(original, assembled):\n",
        "        print(\"✅ Reconstruction MATCHES original!\")\n",
        "    else:\n",
        "        print(\"❌ Reconstruction failed — mismatch detected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    write_test_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7veDLAAZHSnf",
        "outputId": "3170c086-f31d-4a00-8de1-d17d88ed46dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Files created.\n",
            "✅ Reconstruction MATCHES original!\n"
          ]
        }
      ]
    }
  ]
}